{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05063839",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > /home/sat3812/Downloads/mi_preprocess_light.py <<'PY'\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Input and output paths\n",
    "INPUT = \"/home/sat3812/Downloads/mi_complications.csv\"\n",
    "OUT_SINGLE_CSV = \"/home/sat3812/Downloads/mi_clean_light.csv\"\n",
    "REPORT_PATH = \"/home/sat3812/Downloads/mi_clean_light_report.json\"\n",
    "\n",
    "# Spark session (lightweight config for low-RAM VM)\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"mi_preprocess_light\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .config(\"spark.default.parallelism\", \"8\")\n",
    "    .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Loading dataset\n",
    "df = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(INPUT))\n",
    "\n",
    "df.printSchema()\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "\n",
    "# Replace placeholders for missing data\n",
    "placeholders = [\"\", \" \", \"?\", \"NA\", \"NaN\", \"nan\", \"N/A\", \"None\", \"null\", \"NULL\"]\n",
    "df = df.replace(placeholders, None)\n",
    "\n",
    "# Trim string columns\n",
    "string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StringType)]\n",
    "if string_cols:\n",
    "    df = df.select([F.trim(F.col(c)).alias(c) if c in string_cols else F.col(c) for c in df.columns])\n",
    "\n",
    "# Calculate missing ratios\n",
    "row_count = df.count()\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).first().asDict()\n",
    "missing_ratio = {c: (null_counts.get(c, 0) / row_count if row_count else 0.0) for c in df.columns}\n",
    "\n",
    "# Drop columns with >90% missing\n",
    "to_drop = [c for c, r in missing_ratio.items() if r > 0.90]\n",
    "if to_drop:\n",
    "    df = df.drop(*to_drop)\n",
    "\n",
    "# Impute (<40% missing): numeric = mean, string = \"Unknown\"\n",
    "num_cols = [c for c, t in df.dtypes if t in ('double', 'float', 'int', 'bigint')]\n",
    "str_cols = [c for c, t in df.dtypes if t == 'string']\n",
    "to_impute = [c for c, r in missing_ratio.items() if r < 0.40]\n",
    "impute_num = [c for c in to_impute if c in num_cols]\n",
    "impute_str = [c for c in to_impute if c in str_cols]\n",
    "\n",
    "if impute_num:\n",
    "    means = df.select([F.avg(F.col(c)).alias(c) for c in impute_num]).first().asDict()\n",
    "    df = df.na.fill({c: means[c] for c in impute_num if means[c] is not None})\n",
    "if impute_str:\n",
    "    df = df.na.fill({c: \"Unknown\" for c in impute_str})\n",
    "\n",
    "# Normalize yes/no text\n",
    "def norm_bool(col):\n",
    "    return (\n",
    "        F.when(F.lower(F.col(col)).isin(\"true\",\"t\",\"yes\",\"y\",\"1\"), \"true\")\n",
    "         .when(F.lower(F.col(col)).isin(\"false\",\"f\",\"no\",\"n\",\"0\"), \"false\")\n",
    "         .otherwise(F.col(col))\n",
    "    )\n",
    "for c in str_cols:\n",
    "    df = df.withColumn(c, norm_bool(c))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Save single clean CSV\n",
    "tmp_dir = Path(\"/home/sat3812/Downloads/mi_clean_light_tmp\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(str(tmp_dir))\n",
    "part_files = list(tmp_dir.glob(\"part-*.csv\"))\n",
    "if part_files:\n",
    "    if Path(OUT_SINGLE_CSV).exists():\n",
    "        Path(OUT_SINGLE_CSV).unlink()\n",
    "    shutil.move(str(part_files[0]), OUT_SINGLE_CSV)\n",
    "for p in tmp_dir.glob(\"*\"):\n",
    "    p.unlink()\n",
    "tmp_dir.rmdir()\n",
    "\n",
    "# Create and save summary report\n",
    "report = {\n",
    "    \"rows\": row_count,\n",
    "    \"cols\": len(df.columns),\n",
    "    \"dropped_columns_over_90pct_missing\": to_drop,\n",
    "    \"imputed_columns_under_40pct_missing\": to_impute,\n",
    "    \"output_csv\": OUT_SINGLE_CSV\n",
    "}\n",
    "with open(REPORT_PATH, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(json.dumps(report, indent=2))\n",
    "\n",
    "spark.stop()\n",
    "PY"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
